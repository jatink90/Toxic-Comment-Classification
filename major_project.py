# -*- coding: utf-8 -*-
"""Major_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kpJBsseCM-3cFRxGBBeIscGy9osvmsA3

# Importing Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import f1_score
import re
from wordcloud import WordCloud
from wordcloud import STOPWORDS
from sklearn.feature_extraction.text import TfidfVectorizer 
import pickle 
import time
from sklearn.metrics import accuracy_score
from nltk.tokenize import TweetTokenizer 
from nltk import Text
from nltk.tokenize import regexp_tokenize
from nltk.tokenize import word_tokenize  
from nltk.tokenize import sent_tokenize 
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression 
from sklearn.naive_bayes import MultinomialNB
from sklearn.multiclass import OneVsRestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
import string
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline
import nltk
from sklearn.calibration import CalibratedClassifierCV
import gc
from scipy import sparse

!pip install contractions

import contractions

from google.colab import drive
drive.mount("/content/drive")

"""# Dataset Loading"""

train = pd.read_csv('/content/drive/MyDrive/train.csv')
test = pd.read_csv('/content/drive/MyDrive/test.csv')

train

test

train.describe()

test.describe()

train.isnull().sum()

test.isnull().sum()

features = list(train.columns)
features

COMMENT = 'comment_text'
train[COMMENT].fillna("unknown", inplace=True)
test[COMMENT].fillna("unknown", inplace=True)

comments = train.drop(['id','comment_text'],axis = 1)
for i in comments.columns :
    print("Percent of {0}s: ".format(i), round(100*comments[i].mean(),2), "%")

"""# Data Visualisation """

def plot_features_distribution(features, title):
    plt.figure(figsize=(12,6))
    plt.title(title)
    for feature in features:
        sns.distplot(train.loc[~train[feature].isnull(),feature],kde=True,hist=False, bins=120, label=feature)
    plt.xlabel('')
    plt.legend()
    plt.show()

features = list(train.columns)[3:8]
plot_features_distribution(features, "Distribution of toxicity features in the train set")

classes = ["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]

vals1, counts1 = np.unique(train['toxic'] , return_counts=True)
print(vals1, counts1)
#There are 15294 occurances of unwanted words in toxic and 144277 occurances of wanted words in toxic.

vals2, counts2 = np.unique(train['severe_toxic'] , return_counts=True)
print(vals2, counts2)

vals3, counts3 = np.unique(train['obscene'] , return_counts=True)
print(vals3, counts3)

vals4, counts4 = np.unique(train['threat'] , return_counts=True)
print(vals4, counts4)

vals5, counts5 = np.unique(train['insult'] , return_counts=True)
print(vals5, counts5)

vals6, counts6 = np.unique(train['identity_hate'] , return_counts=True)
print(vals6, counts6)

counts = [counts1[1], counts2[1], counts3[1], counts4[1], counts5[1], counts6[1]]

plt.figure(figsize=(10, 5))
colors=['#0066ff', '#ff9900', '#ff0000', '#00cc00', '#6600ff', '#ff0066']
plt.bar(classes, counts, color=colors)
plt.title('Class Distribution')
plt.xlabel('classes')
plt.ylabel('counts')
plt.show()

#Total unwanted words in each classes
for i in range(len(classes)):
    print(f"Total unwanted words in {classes[i]} class: {counts[i]}")

train['comment_text'][0]

lens = train.comment_text.str.len()

lens.mean(), lens.std(), lens.max()

lens.hist()

"""# Preprocessing """

train["text_clean"] = train["comment_text"].apply(lambda x: x.lower())
display(train.head())

train = train.drop(['id'], axis=1)
train

"""## Wordclouds - Frequent words:

"""

stopwords=set(STOPWORDS)

# wordcloud for clean comments
words = ' '.join(train['comment_text'])
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

#Toxic comments
subset = train[train.toxic==1]
text = subset.comment_text.values
wc = WordCloud(background_color="black", max_words=4000, stopwords=stopwords)
wc.generate(" ".join(text))
plt.figure(figsize=(20,20))
plt.subplot(222)
plt.axis("off")
plt.title("Words frequented in Toxic Comments", fontsize=20)
plt.imshow(wc.recolor(colormap='gist_earth', random_state=244), alpha=0.98)

#Severely toxic comments
subset = train[train.severe_toxic==1]
text = subset.comment_text.values
wc = WordCloud(background_color="black", max_words=4000, stopwords=stopwords)
wc.generate(" ".join(text))
plt.figure(figsize=(20,20))
plt.subplot(223)
plt.axis("off")
plt.title("Words frequented in Severe Toxic Comments", fontsize=20)
plt.imshow(wc.recolor(colormap='gist_earth', random_state=244), alpha=0.98)

#Threat comments
subset = train[train.threat==1]
text = subset.comment_text.values
wc = WordCloud(background_color="black", max_words=4000, stopwords=stopwords)
wc.generate(" ".join(text))
plt.figure(figsize=(20,20))
plt.subplot(224)
plt.axis("off")
plt.title("Words frequented in Threat Comments", fontsize=20)
plt.imshow(wc.recolor(colormap='gist_earth', random_state=244), alpha=0.98)

#Insult comments
subset = train[train.insult==1]
text = subset.comment_text.values
wc = WordCloud(background_color="black", max_words=4000, stopwords=stopwords)
wc.generate(" ".join(text))
plt.figure(figsize=(20,20))
plt.subplot(222)
plt.axis("off")
plt.title("Words frequented in Insult Comments", fontsize=20)
plt.imshow(wc.recolor(colormap='gist_earth', random_state=244), alpha=0.98)

#Identity_hate comments
subset = train[train.identity_hate==1]
text = subset.comment_text.values
wc = WordCloud(background_color="black", max_words=4000, stopwords=stopwords)
wc.generate(" ".join(text))
plt.figure(figsize=(20,20))
plt.subplot(221)
plt.axis("off")
plt.title("Words frequented in Identity_hate Comments", fontsize=20)
plt.imshow(wc.recolor(colormap='gist_earth', random_state=244), alpha=0.98)

"""## Text Cleaning"""

train["text_clean"] = train["text_clean"].apply(lambda x: contractions.fix(x))

# double check
print(train["comment_text"][67])
print(train["text_clean"][67])
print(train["comment_text"][12])
print(train["text_clean"][12])

def remove_URL(text):
    return re.sub(r"https?://\S+|www\.\S+", "", text)
train["text_clean"] = train["text_clean"].apply(lambda x: remove_URL(x))

def remove_html(text):
    html = re.compile(r"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});")
    return re.sub(html, "", text)
train["text_clean"] = train["text_clean"].apply(lambda x: remove_html(x))

def remove_non_ascii(text):
    return re.sub(r'[^\x00-\x7f]',r'', text) 
train["text_clean"] = train["text_clean"].apply(lambda x: remove_non_ascii(x))

def remove_punct(text):
    return text.translate(str.maketrans('', '', string.punctuation))
train["text_clean"] = train["text_clean"].apply(lambda x: remove_punct(x))

nltk.download('punkt')

# Tokenizing the tweet base texts.
re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')
def tokenize(s): return re_tok.sub(r' \1 ', s).split()

nltk.download("stopwords")

# Removing stopwords
from nltk.corpus import stopwords
stop = set(stopwords.words('english'))
train['stopwords_removed'] = train["text_clean"].apply(lambda x: [word for word in x if word not in stop])
train.head()

def lancaster_stemmer(text):
    stemmer = nltk.LancasterStemmer()
    stems = [stemmer.stem(i) for i in text]
    return stems

train['lancaster_stemmer'] = train['stopwords_removed'].apply(lambda x: lancaster_stemmer(x))
train.head()

x = train['comment_text']
y = train.iloc[:, 1:7]

x

"""## Train Test Split"""

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13)

X_train

y_train

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

y

"""# Vectorisation """

vect_word = TfidfVectorizer(max_features=20000, tokenizer=tokenize, lowercase=True,strip_accents='unicode',token_pattern=r'\w{1,}', analyzer='word',
                        stop_words= 'english',ngram_range=(1,3),min_df=5,dtype=np.float32)
vect_char = TfidfVectorizer(max_features=40000, tokenizer=tokenize, lowercase=True,strip_accents='unicode',token_pattern=r'\w{1,}', analyzer='char',
                        stop_words= 'english',ngram_range=(3,6),min_df=5,dtype=np.float32)

tr_vect = vect_word.fit_transform(train['comment_text'])
ts_vect = vect_word.transform(test['comment_text'])

# Character n gram vector
tr_vect_char = vect_char.fit_transform(train['comment_text'])
ts_vect_char = vect_char.transform(test['comment_text'])
gc.collect()

X = sparse.hstack([tr_vect, tr_vect_char])
x_test = sparse.hstack([ts_vect, ts_vect_char])

target_col = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']
y = train[target_col]
del tr_vect, ts_vect, tr_vect_char, ts_vect_char
gc.collect()

"""# Model Training

## Logistic Regression
"""

prd = np.zeros((x_test.shape[0],y.shape[1]))
cv_score =[]
for i,col in enumerate(target_col):
    lr = LogisticRegression(C=2,random_state = i,class_weight = 'balanced')
    print('Building {} model for column:{''}'.format(i,col)) 
    lr.fit(X,y[col])
    prd[:,i] = lr.predict_proba(x_test)[:,1]

pred =  lr.predict(X)
print('\nConfusion matrix\n',confusion_matrix(y[col],pred))
print(classification_report(y[col],pred))

prd_1 = pd.DataFrame(prd,columns=y.columns)
submit_lr = pd.concat([test['id'],prd_1],axis=1)
submit_lr.to_csv('submit_lr.csv',index=False)
submit_lr.head()

"""## LinearSVC

"""

prd2 = np.zeros((x_test.shape[0],y.shape[1]))
cv_score2 =[]
for i, col in enumerate(target_col):
    clf = LinearSVC(random_state=i, class_weight='balanced')
    clf.fit(X, y[col])
    clf_calibrated = CalibratedClassifierCV(clf, cv='prefit', method='sigmoid')
    clf_calibrated.fit(X, y[col])
    prd2[:, i] = clf_calibrated.predict_proba(x_test)[:, 1]

pred2 =  clf.predict(X)
print('\nConfusion matrix\n',confusion_matrix(y[col],pred2))
print(classification_report(y[col],pred2))

prd_2 = pd.DataFrame(prd2,columns=y.columns)
submit_svc = pd.concat([test['id'],prd_2],axis=1)
submit_svc.to_csv('submit_svc.csv',index=False)
submit_svc.head()

"""## RandomForest Classifier"""

pipe = make_pipeline(TfidfVectorizer(stop_words='english',
                                     strip_accents='unicode',
                                     token_pattern=r'\w{1,}',
                                     analyzer='word',
                                     ngram_range=(1, 1),
                                     min_df=5),
                     OneVsRestClassifier(RandomForestClassifier()))

param_grid = {'tfidfvectorizer__max_features': [10000, 30000],
              'onevsrestclassifier__estimator__n_estimators': [100, 200],
              'onevsrestclassifier__estimator__max_depth': [5, 10],
             } 

grid = GridSearchCV(pipe, param_grid, cv=3, scoring='roc_auc')
grid_2 = grid.fit(X_train, y_train)

# Make predictions on the test set
y_pred = grid_2.predict(X_test)

# Calculate the accuracy score
accuracy = accuracy_score(y_test, y_pred)

print('Accuracy: {:.2f}%'.format(accuracy*100))

holdout_comments = test['comment_text']
# holdoutComments are automatically transformed throguh the grid3 pipeline before prodicting probabilities
twod = grid_2.predict_proba(holdout_comments)
holdout_predictions = {}
holdout_predictions = {'id': test['id']}  

holdout_predictions['toxic']=twod[:,0]
holdout_predictions['severe_toxic']=twod[:,1]
holdout_predictions['obscene']=twod[:,2]
holdout_predictions['threat']=twod[:,3]
holdout_predictions['insult']=twod[:,4]
holdout_predictions['identity_hate']=twod[:,5]
    
sub_rfc = pd.DataFrame.from_dict(holdout_predictions)
sub_rfc = sub_rfc[['id','toxic','severe_toxic','obscene','threat','insult','identity_hate']] #rearrange columns
sub_rfc.to_csv('submit_rfc.csv', index=False)

submit_rfc = pd.read_csv('submit_rfc.csv')
submit_rfc.head()

"""# Submissions """

submit_lr

submit_svc

submit_rfc

